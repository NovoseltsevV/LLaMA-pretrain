## Предобучение LLaMA с нуля

В этом проекте реализована модель трансформера на ~200M параметров, вдохновлённая оригинальной статьёй [LLaMA](https://arxiv.org/abs/2302.13971).

Архитектура включает современные улучшения:
- RoPE (Rotary Position Embeddings)
- SwiGLU
- RMSNorm
- Pre-normalization

Модель обучалась с нуля на одной восьмой датасета [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/), всего было обработано **250М токенов**.

---

## Графики обучения

Обучение проводилось на одной видеокарте в Google Colab и ограничением по времени 4 часа. Поэтому каждый цвет на графиках — это запуск обучения в новом сеансе c сохранением и продолжением обучения из чекпоинтов, сохранённых на Google Drive.

<p align="center">
  <img src="images/training_curves.png" alt="Графики обучения" width="800"/>
</p>

---

## Структура репозитория

- `model.py` — классы архитектуры модели
- `train_loop.py` — тренировочный цикл:
  - остановка по количеству токенов
  - накопление градиентов
  - смешанная точность (`torch.autocast`)
  - сохранение и загрузка чекпоинтов
- `data_preprocess.py` — подготовка данных:
  - объединение строк из батча в одну строку
  - разбиение полученной строки на последовательности максимальной длины без паддингов
  - создание `DataLoader`
- `drive_managment.py` — очистка корзины Google Drive из Google Colab

---

## Особенности обучения

- Использование `torch.autocast` для экономии памяти
- FlashAttention для ускорения внимания
- Трекер обучения — [Weights & Biases](https://wandb.ai)
- Все сессии запускались на Google Colab, с сохранением прогресса

---

## Планы

- Увеличение длины контекста модели с помощью [Position Interpolation](https://arxiv.org/pdf/2306.15595)
- Оценка на [tinyMMLU](https://huggingface.co/datasets/tinyBenchmarks/tinyMMLU) бенчмарке
